{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Word Embedding's\n",
    "# Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to show a very simple implimentation for creating word embeddings. Following that, it will demonstrate examples of how the word semantics (meaning) is caputred in word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# Numerical python library\n",
    "import numpy as np\n",
    "# Dimensionality Reduction to visualise high dimensional data\n",
    "from sklearn.manifold import TSNE\n",
    "# Library to create visualisations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "Read the book into a python list, each element of the list is one line from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('books/a_tale_of_two_cities.txt', 'r') as ins:\n",
    "\tbook_list = []\n",
    "\tfor line in ins:\n",
    "\t\tbook_list.append(line)\n",
    "\n",
    "# add more books\n",
    "other_books = ['books/1400-0.txt', 'books/766-0.txt', 'books/786-0.txt',\n",
    "               'books/pg1023.txt', 'books/pg730.txt']\n",
    "\n",
    "for book_fn in other_books:\n",
    "    with open(book_fn, 'r') as ins:\n",
    "        for line in ins:\n",
    "            book_list.append(line)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 5 elements of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A TALE OF TWO CITIES\\n', '\\n', 'A STORY OF THE FRENCH REVOLUTION\\n', '\\n', 'By Charles Dickens\\n', '\\n', '\\n', 'Book the First--Recalled to Life\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(book_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that each element ends with '\\n'. This is the string representation for new line in Python. Next step, remove the new line string and concatenate text where there are no empty lines in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = []\n",
    "current_line = book_list[0].replace('\\n', '')\n",
    "for i in range(1, len(book_list)-1):\n",
    "\tline = book_list[i].replace('\\n', '')\n",
    "\tif len(line) > 0:\n",
    "\t\tcurrent_line = current_line + ' ' + line\n",
    "\telse:\n",
    "\t\tif len(current_line) > 0:\n",
    "\t\t\tbook.append(current_line)\n",
    "\t\tcurrent_line = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 6 elements of our new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TALE OF TWO CITIES\n",
      " A STORY OF THE FRENCH REVOLUTION\n",
      " By Charles Dickens\n",
      " Book the First--Recalled to Life\n",
      " I. The Period\n",
      " It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way-- in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 6):\n",
    "    print(book[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: We want to split each of the elements of this list into individual words, also here we make the decision for the purpose of simplicity to remove all non alpha numeric characters from the text and to convert all text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_book = []\n",
    "for i in range(0, len(book)):\n",
    "\t# replace all punctuation with a space\n",
    "\tline = book[i].lower()\n",
    "\tline = re.sub('[^a-z0-9]', ' ', line)\n",
    "\t# tokenise all words\n",
    "\ttokens = line.split()\n",
    "\ttoken_book.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'tale', 'of', 'two', 'cities']\n",
      "['a', 'story', 'of', 'the', 'french', 'revolution']\n",
      "['by', 'charles', 'dickens']\n",
      "['book', 'the', 'first', 'recalled', 'to', 'life']\n",
      "['i', 'the', 'period']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(token_book[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily use this nested list of words to make a dictionary of word counts. Which we will use to determine which words we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "for line in token_book:\n",
    "\tfor token in line:\n",
    "\t\ttry:\n",
    "\t\t\tword_counts[token] += 1\n",
    "\t\texcept:\n",
    "\t\t\tword_counts[token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"a\" appears 28692 times in this book\n",
      "The word \"tale\" appears 18 times in this book\n",
      "The word \"of\" appears 32083 times in this book\n",
      "The word \"two\" appears 1511 times in this book\n",
      "The word \"cities\" appears 4 times in this book\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in word_counts:\n",
    "    print('The word', '\"' + key + '\"', 'appears', word_counts[key], 'times in this book')\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These books have 25099 unique words.\n"
     ]
    }
   ],
   "source": [
    "print('These books have', len(word_counts), 'unique words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Co-Occurence Matrix\n",
    "Now that our data is ready we can create a Co-occurence matrix\n",
    "\n",
    "First we get list of words that appear 70 or more times in the books and assign each word a unique index which corresponds to which row and column of the matrix the word will appear. We will only use the words which appear atleast 70 times so that there are enough occurances of each word to get a good idea for the types of context the word is used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "word_index = {}\n",
    "for k in word_counts:\n",
    "\tif word_counts[k] >= 70:\n",
    "\t\tword_index[k] = i\n",
    "\t\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix of zeros where each row and column are representative of one of the words in the word index created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1581, 1581)\n"
     ]
    }
   ],
   "source": [
    "w_n = len(word_index)\n",
    "# initialise matrix of zeros\n",
    "com = np.zeros((w_n, w_n))\n",
    "\n",
    "print(com.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our matrix of zeros, we go through each paragraph and and store the context's of each word. \n",
    "\n",
    "This is done as seen in the following example:\n",
    "\n",
    "Here we have an example sentence and an empty matrix, like we created above.\n",
    "\n",
    "\n",
    "![alt text](stills/w_e_1.png \"Title\")\n",
    "\n",
    "We assign a window size 'w'. Here we chose 'w = 3'. We start at the position 'w + 1', this is our first centre word. Our window is made up of the 'w' words that come before and after our centre word\n",
    "\n",
    "\n",
    "![alt text](stills/w_e_2.png \"Title\")\n",
    "\n",
    "\n",
    "Now we have our first centre word and window, go to the matrix, for the row of that centre word, we add a count +1 for each of the columns of the window words. Note, if a word appears which is not in our matrix, we do nothing and move on\n",
    "\n",
    "![alt text](stills/w_e_3.png \"Title\")\n",
    "\n",
    "\n",
    "Next we slide our centre word and window down one and run the same operation to make the additions to the matrix\n",
    "\n",
    "![alt text](stills/w_e_4.png \"Title\")\n",
    "\n",
    "We do this for word in the paragraph until we reach the end of the paragraph. We do this for each paragraph in the corpus until we reach the end of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set window size - w - how many words in the context of the centre word\n",
    "w = 3\n",
    "\n",
    "# move sliding window for all lines, add co-occurences to the matrix\n",
    "for line in token_book:\n",
    "\twords_in_line = len(line)\n",
    "\tif words_in_line >= 2*w+1:\n",
    "\t\tfor i in range(w, words_in_line-w):\n",
    "\t\t\tc = line[i]\n",
    "\t\t\twindow_words = line[i-w:i] + line[i+1:i+1+w]\n",
    "\t\t\tfor ww in window_words:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\trow = word_index[c]\n",
    "\t\t\t\t\tcol = word_index[ww]\n",
    "\t\t\t\t\tcom[row, col] += 1\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 darkness\n",
      "2 hope\n",
      "3 we\n",
      "4 had\n",
      "5 everything\n",
      "\n",
      "\n",
      "[[  0   0   0   4   0]\n",
      " [  0   0  13  27   1]\n",
      " [  0  13  83 377   7]\n",
      " [  4  25 377 441  16]\n",
      " [  0   1   8  16   2]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(22, 27):\n",
    "    for word in word_index:\n",
    "        if word_index[word] == i:\n",
    "            print(i-21, word)\n",
    "print('\\n')\n",
    "print(com[22:27, 22:27].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring word embeddings\n",
    "\n",
    "We can now take the row for each word and that will be a vector which we can use to represent that word. Each word vector (or word embedding) contrains information gained about the words which are commonly used around that word. One feature of this is that we can now see words which are similar types of words. The word vectors we have here are 1581 (one element for each word which we were counting).\n",
    "\n",
    "As these vectors are of high dimensionality. We cannot visualise them as they are. What we can do is reduce the dimensionality either by Principal Component Analysis (PCA) or by t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "Here we can look at some different groups of words and get some clues and about how these word embeddings <b>might</b> storing information about the meaning of the word.\n",
    "\n",
    "Here we can see different groups of words, in the top left we have body parts, top right we have country related words, and to the right we have peoples names.\n",
    "\n",
    "![alt text](stills/we_vis_1.png \"Title\")\n",
    "\n",
    "\n",
    "In this example we can look at the relationship between words, here we look at comparing a word in its plural form to a word in its singular form, with examples of 'women' to 'woman' and going from 'men' to 'man'. We can see that the vectors between these examples (represented here by the red arrows) are similar. I think this is a good example in that it is not exactly the same, this is important to remember because firstly we have reduced a 1581 dimensional space to a 2 dimensional space, so information will be lost here, and secondly, the process of creating word embeddings is not perfect. But that does not mean that these word embeddings are not very useful.\n",
    "\n",
    "\n",
    "![alt text](stills/we_vis_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
